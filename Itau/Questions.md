### Desafio 1

Considerando os dados do arquivo dados_Q1.csv, treine o modelo ‚Äú√Årvore de Classifica√ß√£o‚Äù (fun√ß√£o de custo: entropia, sem podas) e utilize como metodologia de reamostragem a t√©cnica valida√ß√£o cruzada sequencial (com 10 folds). 

a) Qual √© a m√©dia da medida acur√°cia nas parti√ß√µes de treino? 

[1.00]

b) Qual √© a m√©dia da medida acur√°cia nas parti√ß√µes de valida√ß√£o?

[0.76]

### Desafio 2

Considerando os dados do arquivo dados_Q2.csv, treine o modelo SVM (kernel: linear, fator de regulariza√ß√£o: 0.1) e utilize como metodologia de reamostragem a t√©cnica Leave-One-Out.

a) Qual √© a m√©dia da medida MAE nas parti√ß√µes de treino?

[76.91]

b) Qual √© a m√©dia da medida MAE nas parti√ß√µes de valida√ß√£o?

[78.39]

### Desafio 3

Considerando os dados do arquivo dados_Q3.csv, treine o modelo Regress√£o Log√≠stica (tipo de regulariza√ß√£o: L1, fator de regulariza√ß√£o: 0.1) utilizando a metodologia de valida√ß√£o holdout. Use para treino a primeira metade da base de dados, e as demais linhas para valida√ß√£o. Para fins de aproxima√ß√£o, considere que n√∫meros menores que 0.001 s√£o iguais a zero.

a) Qual √© o valor da medida F1 Score para a parti√ß√£o de valida√ß√£o?

[0.61]

b) Quantas vari√°veis influenciam as predi√ß√µes geradas pelo modelo treinado?

['v_11', 'v_10', 'v_3', 'v_16', 'v_12']

### Desafio 4

Considerando os dados do arquivo dados_Q4.csv, aplique a t√©cnica PCA (Principal Component Analysis)

a) Qual √© a fra√ß√£o da variabilidade total explicada pelo segundo componente principal?

[33.73]

b) Qual √© o n√∫mero m√≠nimo de componentes necess√°rios para explicar 99% da variabilidade dos dados?

[8]

### Desafio 5

A) Considerando os dados do arquivo dados_Q5.csv, execute o algoritmo Complete-Linkage e corte o dendrograma na altura 2,3. Qual a posi√ß√£o do centroide de cada grupo formado? 

[[-0.01067531, 0.04420797, -0.04634375, 0.88997719, 0.06051477]
 [1.02705476, 0.02465894, -0.00805571, -0.0338859, -0.07312192]
 [-0.0343138, 1.11416901, 0.00533692, 0.03115574, 0.03287099]
 [-0.02287197, -0.01309619, 0.98219399, 0.07248944, 0.06414487]
 [0.04243589, 0.13170493, 0.12078695, 0.01712429, -0.02424086]]

B) Considerando os dados do arquivo dados_Q5.csv, execute o algoritmo Kmeans (com 10 itera√ß√µes no m√°ximo). Utilize como posi√ß√£o inicial dos centroides a posi√ß√£o calculada na quest√£o anterior. Qual a posi√ß√£o final dos centroides?

[[6.63286061e-02, 3.61618954e-02, -3.61446512e-02, 1.03064205e+00, 3.19803265e-02]
 [1.01085001e+00, 1.41187280e-02, -1.52992669e-02, -2.02804403e-02, -5.56874047e-02]
 [-3.18851265e-02, 1.02291983e+00, 6.40807866e-02, 9.06345310e-03, 4.93184937e-02]
 [2.40435798e-03, 1.50286066e-02, 1.02150177e+00, 4.09546617e-02, 3.78730831e-02]
 [-7.54894132e-02, -3.42770553e-02, 1.41084792e-04, 1.07591849e-01, -2.13484714e-03]]


Bloco de Perguntas 1

Assinale as alternativas com V ou F para Verdadeiro ou Falso respectivamente.

Atente para o fato que uma quest√£o errada anula uma certa.
Em caso de d√∫vidas deixe em branco.

3 - √â poss√≠vel calcular eficientemente as dist√¢ncias entre as inst√¢ncias mais pr√≥ximas utilizando KD-Trees, que funciona melhor quando a dimens√£o das inst√¢ncias √© pequena.

[V]

4 - Quando falamos de similaridade cosseno em vari√°veis independentes positivas (como TF-IDF, por exemplo) sabemos que deve estar no intervalo [0, 1], em que 0 significa a maior dist√¢ncia.

[V]

5 - O objetivo do algoritmo k-means √© minimizar a soma dos quadrados das dist√¢ncias dos pontos aos centroides.

[F]

6 - Diferente do k-means com dist√¢ncia euclidiana, o GMM (Gaussian Mixture Models) pode encontrar grupos com formato elipsoidal.

[V]

7 - As t√©cnicas PCA e T-SNE s√£o amplamente utilizadas porque conseguem reduzir o n√∫mero de dimens√µes de uma base de dados, sem que haja perda de informa√ß√µes.

[V]

8 - Silhueta √© um √≠ndice que mensura a qualidade do agrupamento a qual favorece uma menor dist√¢ncia intracluster e uma grande dist√¢ncia extracluster.

[V]

9 - O dendrograma permite visualizar os grupos formados em diferentes granularidades, sem ser necess√°rio executar novamente o algoritmo.

[V]

10 - H√° dois tipos principais de algoritmos para Agrupamento hier√°rquico: Divisivos (ou top-down) e Aglomerativos (ou botton-up).

[V]

11 - O m√©todo SGD (Stochastic Gradient Descendent) √© utilizado para treinar diversos tipos de modelos, tais como redes neurais, regress√£o log√≠stica e SVM.

[V]

12 - No paradigma Map/Reduce, remover caracteres especiais de cada documento de um corpus (cole√ß√£o de documentos) √© um exemplo de Reduce.

[F]


Bloco de Perguntas 2

Assinale as alternativas com V ou F para Verdadeiro ou Falso respectivamente.
Atente para o fato que uma quest√£o errada anula uma certa.
Em caso de d√∫vidas deixe em branco.


3 -  A regress√£o log√≠stica utiliza o m√©todo de M√≠nimos Quadrados Ordin√°rios na estima√ß√£o dos par√¢metros.

[F]

4 - Na regress√£o log√≠stica, a vari√°vel dependente √© cont√≠nua no intervalo entre [0,1].

[V]

5 - Na regress√£o log√≠stica as vari√°veis independentes podem ser num√©ricas ou categ√≥ricas. O coeficiente associado a cada vari√°vel representa a varia√ß√£o do valor estimado quando esta aumenta em 1 unidade.

[V]

6 - Em uma √°rvore de decis√£o pode-se avaliar a taxa de crescimento do erro de classifica√ß√£o para paralisar a divis√£o dos ramos.

[V]

7 - No classificador SVM, pode-se alterar a fun√ß√£o objetivo para permitir que alguns pontos caiam dentro da margem durante o treinamento.

[V]

8 - O precision √© uma medida de qualidade calculada pela fra√ß√£o de quantas predi√ß√µes positivas est√£o corretas, podendo ser utilizada tanto para algoritmos de classifica√ß√£o quanto de regress√£o.

[F]

9 - O KNN, com dist√¢ncia euclidiana, √© sens√≠vel √† escala dos atributos usado para trein√°-lo.

[V]

10 - No processo de cria√ß√£o de ensembles, quanto mais correlacionadas forem as predi√ß√µes individuais de cada classificador, o ganho de desempenho do modelo final combinado tende a ser menor.

[F]

11 - O algoritmo Naive Bayes assume depend√™ncia entre cada par de caracter√≠stica.

[F]

12 - Combinar diversas √°rvores de decis√£o usando a t√©cnica Bagging √© o mesmo que treinar uma Random Forest.

[V]

13 - Os m√©todos de ensemble com boosting combinam v√°rios classificadores para produzir um classificador mais robusto.

[V]

14 - Uma das formas de tratar problemas de classes desbalanceadas √© incluir a informa√ß√£o a priori do n√∫mero de amostras na fun√ß√£o de custo a ser otimizada.

[Pula]

15 - N√£o √© aconselhado a utiliza√ß√£o da acur√°cia para problemas com classes desbalanceadas.

[V]


Bloco de Perguntas 3

2 - Para problemas de regress√£o com duas vari√°veis independentes altamente correlacionadas, enquanto o Lasso tem maior chance de escolher aleatoriamente uma delas, o m√©todo Elastic Net pode selecionar ambas.

[Pula]

3 - L1 e L2 s√£o ambos componentes de regulariza√ß√£o cujo objetivo √© tratar o overfitting exclusivamente para problemas de regress√£o.

[F]

4 - Em um problema de regress√£o utilizando KNN, quanto maior for o n√∫mero de vizinhos (K), menor a probabilidade de overfitting.

[V]

5 - O bootstrap pode ser usado para estimar os erros dos coeficientes de uma regress√£o Linear.

[V]

6 - Grid Search √© uma t√©cnica estoc√°stica para encontrar os melhores hiperpar√¢metros para um modelo.

[V]

Estatistica

a) Sob o contexto de infer√™ncia estat√≠stica e teste de hip√≥teses, calcule um intervalo de confian√ßa para a m√©dia do vetor [4, 13, 9, 9, 8, 8, 11, 10, 12, 7, 20, 11, 7, 16, 13] utilizando 1.000 reamostras de bootstrap. Considere ùëß(1‚àíùõº 2 ‚ÅÑ ) = 1,96. 

[8.53333333, 12.53333333]

b) Uma gerente deve decidir sobre a concess√£o de empr√©stimos aos seus clientes. Para tal, ela deve analisar diversas informa√ß√µes para definir a chance do cliente ficar inadimplente. Com base em dados passados, ela estima em 15% a taxa de inadimpl√™ncia. Dentre os inadimplentes, ela tem 90% de chance de tomar a decis√£o certa, enquanto essa chance aumenta para 95% entre os clientes adimplentes. Essa gerente acaba de recusar um empr√©stimo, qual √© a probabilidade da decis√£o tomada estar correta? 

15% taxa de inadimpl√™ncia
85% taxa de adimpl√™ncia

Inadimplentes 90% de chance decis√£o correta
Adimplentes 95% de chance

0,15 * 0,9 + 0,85 * 0,95 = 0,9425 = 94,25%

2 - Considerando tr√™s eventos quaisquer A, B e C, com probabilidade de ocorr√™ncia maior que zero, pode-se afirmar que se os eventos forem independentes ent√£o P (A ‚à© B ‚à© C) = P(A ‚à© B) + P(C).

A afirma√ß√£o n√£o √© correta,

Tendo A, B e C como independentes:

P(A ‚à© B) = P(A)P(B), P(B ‚à© C) = P(B)P(C), P(A ‚à© C) = P(A)P(C) 
P(A ‚à© B ‚à© C) = P(A)P(B)P(C)

Ent√£o, P(A ‚à© B ‚à© C) = P(A)P(B)P(C) = P(A ‚à© B)P(C).

3 -  Se dois eventos s√£o disjuntos, ent√£o P(A ‚à© B) = P(A) x P(B).

Afirma√ß√£o n√£o √© correta, para que P(A ‚à© B) seja igual P(A) x P(B) os elementos A e B precisam ser independentes.



