{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_modelo_lstm_sequential.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSFCRw31Fwpg2n1qp9OwkS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fernandofsilva/Challenges/blob/master/notebooks/04_modelo_lstm_sequential.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGnVfC8rlW2L"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5164wKgmlQ7V"
      },
      "source": [
        "#@title Carregando as bibliotecas base\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "\n",
        "seaborn.set_style('whitegrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBLRJNLJla7l"
      },
      "source": [
        "#@title Carregando os dados\n",
        "data = pd.read_csv(f'/content/drive/My Drive/Mestrado/data/dados_treino_teste.csv.gz', compression='gzip', index_col=0)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwInelNJlem8"
      },
      "source": [
        "# Engenharia das variáveis (Feature Engineering)\n",
        "\n",
        "Essa sessão é composta da transformação dos dados para entrada na rede na rede neural. Portando, as variáveis são transformadas do seu valor original, seja para adequação dentro da rede neural ou para um melhor treinamento da rede, essas transformações são:\n",
        "\n",
        "- Variavéis númericas: preco_exercicio, preco_ativo, foram normalizadas antes da entrada na rede\n",
        "- Variavéis númericas: preco_opcao (alvo), volatilidade, taxa_juros e T não sofreram alterações\n",
        "- Variável categórica mercado sofreu one hot encoding\n",
        "\n",
        "A transformação dos dados é feita no mesmo momento que o modelo é treinado, isso é feito através de uma camada dentro do modelo, essa camada tem o nome de feature layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZV1T2U2lcft"
      },
      "source": [
        "#@title Pipeline de entrada dos dados\n",
        "def df_to_dataset(dataframe, base, shuffle=True, batch_size=22):\n",
        "\n",
        "    # Criar cópia do dataframe\n",
        "    dataframe = dataframe.copy()\n",
        "\n",
        "    # Filtrar a base\n",
        "    dataframe = dataframe[dataframe['base'] == base]\n",
        "\n",
        "    # Variavel alvo\n",
        "    labels = dataframe.pop('preco_opcao')\n",
        "\n",
        "    # Colunas do modelo\n",
        "    cols = ['mercado', 'preco_exercicio', 'preco_ativo', 'T', 'volatilidade', 'taxa_juros']\n",
        "\n",
        "    # Criar o td.data\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe[cols]), labels))\n",
        "\n",
        "    # Embaralhar os dados se necessário\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "\n",
        "    # Criar o batch de dados\n",
        "    ds = ds.batch(batch_size)\n",
        "\n",
        "    return ds\n",
        "\n",
        "# Divisão da base de treino e teste\n",
        "train_ds = df_to_dataset(data, base='treino')\n",
        "test_ds = df_to_dataset(data, shuffle=False, base='teste')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQI_Vr81ljmL"
      },
      "source": [
        "#@title Mapeamento das colunas\n",
        "feature_columns = []\n",
        "\n",
        "# Colunas númericas normalizadas\n",
        "for column in ['preco_exercicio', 'preco_ativo']:\n",
        "    \n",
        "    mean = data.loc[data['base'] == 'treino', column].mean()\n",
        "    stdev = data.loc[data['base'] == 'treino', column].std()\n",
        "\n",
        "    feature_columns.append(tf.feature_column.numeric_column(column, normalizer_fn = lambda x: (x - mean) / stdev))\n",
        "\n",
        "# Colunas númericas sem normalização\n",
        "for column in ['T', 'volatilidade', 'taxa_juros']:\n",
        "\n",
        "    feature_columns.append(tf.feature_column.numeric_column(column))\n",
        "\n",
        "# Colunas categóricas\n",
        "option = tf.feature_column.categorical_column_with_vocabulary_list('mercado', ['OPÇÕES DE COMPRA', 'OPÇÕES DE VENDA'])\n",
        "option_one_hot = tf.feature_column.indicator_column(option)\n",
        "feature_columns.append(option_one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwxWWjEdlmU9"
      },
      "source": [
        "#@title Camada de transformação (feature layer)\n",
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns, name='Feature')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EVQFSnFlm0x"
      },
      "source": [
        "# Modelo\n",
        "\n",
        "O modelo de rede neural profunda a seguir, foi baseado nos estudos desenvolvidos por Hirsa, Karatas, & Oskoui. No trabalho são testadas diversas arquiteturas (camadas e elementos em cada camada), bem como função de atição de cada camada e também função de otimização.\n",
        "\n",
        "A conclusão do estudo mostra que os melhores resultados foram obtidos utilizando uma rede de 4 camadas com 120 neurônios cada uma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV7Y0AeYlprW"
      },
      "source": [
        "#@title Criar, compilar o modelo\n",
        "# Define de model\n",
        "def get_model():\n",
        "    model = tf.keras.Sequential([\n",
        "    feature_layer,\n",
        "    tf.keras.layers.Dense(120, activation='relu'),\n",
        "    tf.keras.layers.Dense(120, activation='relu'),\n",
        "    tf.keras.layers.Dense(120, activation='relu'),\n",
        "    tf.keras.layers.Dense(120, activation='elu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "        loss='mse',\n",
        "        metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"MAE\", dtype=None)]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-mYnwmvlq4e"
      },
      "source": [
        "#@title Callbacks\n",
        "# Tensorflow checkpoint\n",
        "path = '/content/drive/MyDrive/Mestrado/models/dnn/'\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=path+'checkpoint',\n",
        "    frequecy='epoch',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",factor=0.2, min_delta=0.001, patience=5, verbose=1)\n",
        "\n",
        "csv = tf.keras.callbacks.CSVLogger(path+\"results.csv\")\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWoEsxYZltpZ"
      },
      "source": [
        "#@title Treinar o modelo\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=200,\n",
        "    callbacks=[checkpoint, lr, csv, es]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp87ReOXlvbV"
      },
      "source": [
        "#@title Métricas\n",
        "metrics = pd.read_csv(path + 'results.csv')\n",
        "metrics.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_AcFaLMlw7g"
      },
      "source": [
        "#@title Plot das Métricas do Modelo\n",
        "\n",
        "# Valor de mudança de learning rate\n",
        "metrics['lr_change'] = metrics['lr'] == metrics['lr'].shift(1)\n",
        "\n",
        "# Valores de learning para plot de gráfico\n",
        "lr_change = metrics.loc[metrics['lr_change'] == False, ['epoch', 'lr']]\n",
        "\n",
        "# Retirar a primeira linha\n",
        "lr_change = lr_change[1:]\n",
        "\n",
        "# Converter para dict\n",
        "lr_change = lr_change.to_dict('records')\n",
        "\n",
        "# Create two subplots\n",
        "fig, axs = plt.subplots(2, 1, figsize=(20, 8))\n",
        "axs[0].plot(metrics['epoch'], metrics['loss'], 'tab:blue', label='treino')\n",
        "axs[0].plot(metrics['epoch'], metrics['val_loss'], 'tab:red', label='teste')\n",
        "axs[0].set_title('Função de Perda')\n",
        "axs[0].set(ylabel='Perda')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(metrics['epoch'], metrics['MAE'], 'tab:blue', label='treino')\n",
        "axs[1].plot(metrics['epoch'], metrics['val_MAE'], 'tab:red', label='teste')\n",
        "axs[1].set_title('Erro Médio Absoluto')\n",
        "axs[1].set(ylabel='MAE')\n",
        "axs[1].legend()\n",
        "\n",
        "for lr in lr_change:\n",
        "    axs[0].axvline(x=lr['epoch'], linestyle='dotted', color='black')\n",
        "    axs[1].axvline(x=lr['epoch'], linestyle='dotted', color='black')\n",
        "    axs[0].text(\n",
        "        x=lr['epoch'],\n",
        "        y=0.1,\n",
        "        s=f\"lr reduzido para: {lr['lr']:.8f}\",\n",
        "        horizontalalignment='right'\n",
        "        )\n",
        "    axs[1].text(\n",
        "        x=lr['epoch'],\n",
        "        y=0.1,\n",
        "        s=f\"lr reduzido para: {lr['lr']:.8f}\",\n",
        "        horizontalalignment='right'\n",
        "        )\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.set(xlabel='Épocas')\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyS4Wt1i-ZVM"
      },
      "source": [
        "#@title Carregar o melhor modelo\n",
        "model = get_model()\n",
        "model.load_weights(path+'checkpoint')\n",
        "\n",
        "# Predição na base de teste\n",
        "prediction = model.predict(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzri3Hgh-a5W"
      },
      "source": [
        "#@title Salvar predição\n",
        "np.savez_compressed('/content/drive/My Drive/Mestrado/data/predicao_dnn.npz', prediction)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}